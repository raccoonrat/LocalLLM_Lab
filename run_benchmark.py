#!/usr/bin/env python3
"""
LocalLLM_Lab - è‡ªåŠ¨åŒ–åŸºå‡†æµ‹è¯•è„šæœ¬
è¿è¡Œå¤šç»„å®éªŒå¹¶æ”¶é›†æ€§èƒ½æ•°æ®
"""

import os
import sys
import subprocess
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# é…ç½®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent
BUILD_DIR = PROJECT_ROOT / "build"
MODELS_DIR = PROJECT_ROOT / "models"
LOGS_DIR = PROJECT_ROOT / "logs"

# å¯æ‰§è¡Œæ–‡ä»¶åï¼ˆWindows ä¸º .exeï¼ŒLinux/MacOS æ— æ‰©å±•åï¼‰
EXE_EXT = ".exe" if sys.platform == "win32" else ""
LLAMA_CLI = BUILD_DIR / f"llama-cli{EXE_EXT}"

# æµ‹è¯•æç¤ºè¯
TEST_PROMPT = "Write a short story about a robot learning to paint."

# æ¨¡å‹æ–‡ä»¶
MAIN_MODEL = MODELS_DIR / "Phi-3.5-mini-instruct-Q4_K_M.gguf"
DRAFT_MODEL = MODELS_DIR / "Qwen2-0.5B-Instruct-Q4_K_M.gguf"


def ensure_dirs():
    """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
    for dir_path in [BUILD_DIR, MODELS_DIR, LOGS_DIR]:
        dir_path.mkdir(parents=True, exist_ok=True)


def check_prerequisites() -> bool:
    """æ£€æŸ¥å‰ç½®æ¡ä»¶"""
    if not LLAMA_CLI.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {LLAMA_CLI}")
        print(f"   è¯·å…ˆè¿è¡Œ build_llama.ps1 ç¼–è¯‘ llama.cpp")
        return False
    
    if not MAIN_MODEL.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {MAIN_MODEL}")
        print(f"   è¯·å…ˆè¿è¡Œ download_models.ps1 ä¸‹è½½æ¨¡å‹")
        return False
    
    return True


def run_benchmark(
    name: str,
    threads: int = 4,
    kv_cache_type: Optional[str] = None,
    batch_size: int = 512,
    draft_model: Optional[Path] = None,
    n_predict: int = 128,
    n_prompt: int = 128
) -> Dict:
    """
    è¿è¡Œå•æ¬¡åŸºå‡†æµ‹è¯•
    
    Args:
        name: å®éªŒåç§°
        threads: CPU çº¿ç¨‹æ•°
        kv_cache_type: KV Cache ç±»å‹ (None=é»˜è®¤, "q8_0"=8-bit)
        batch_size: æ‰¹å¤„ç†å¤§å°
        draft_model: Draft Model è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        n_predict: ç”Ÿæˆ token æ•°é‡
        n_prompt: æç¤ºè¯ token æ•°é‡
    """
    print(f"\n{'='*60}")
    print(f"ğŸ§ª è¿è¡Œå®éªŒ: {name}")
    print(f"{'='*60}")
    
    # æ„å»ºå‘½ä»¤
    cmd = [
        str(LLAMA_CLI),
        "-m", str(MAIN_MODEL),
        "-p", TEST_PROMPT,
        "-t", str(threads),
        "-n", str(n_predict),
        "--batch-size", str(batch_size),
        "--ctx-size", "2048",
        "--log-disable",
    ]
    
    # KV Cache é‡åŒ–
    if kv_cache_type:
        cmd.extend(["-ctk", kv_cache_type, "-ctv", kv_cache_type])
    
    # Draft Model (æŠ•æœºé‡‡æ ·)
    if draft_model and draft_model.exists():
        cmd.extend(["--draft", str(draft_model)])
    
    # è¿è¡Œæµ‹è¯•
    start_time = time.time()
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶
            encoding='utf-8',
            errors='ignore'
        )
        elapsed = time.time() - start_time
        
        # è§£æè¾“å‡º
        output = result.stdout + result.stderr
        metrics = parse_output(output, elapsed, name)
        
        print(f"âœ… å®Œæˆ: {name}")
        print_metrics(metrics)
        
        return metrics
        
    except subprocess.TimeoutExpired:
        print(f"âŒ è¶…æ—¶: {name}")
        return {"name": name, "error": "timeout"}
    except Exception as e:
        print(f"âŒ é”™è¯¯: {name} - {e}")
        return {"name": name, "error": str(e)}


def parse_output(output: str, elapsed: float, name: str) -> Dict:
    """è§£æ llama.cpp è¾“å‡ºï¼Œæå–æ€§èƒ½æŒ‡æ ‡"""
    metrics = {
        "name": name,
        "elapsed_time": elapsed,
        "prompt_tokens": 0,
        "generated_tokens": 0,
        "prompt_tokens_per_sec": 0.0,
        "generated_tokens_per_sec": 0.0,
        "total_tokens_per_sec": 0.0,
    }
    
    # å°è¯•ä»è¾“å‡ºä¸­æå– token ç»Ÿè®¡
    lines = output.split('\n')
    for line in lines:
        line_lower = line.lower()
        
        # æŸ¥æ‰¾ token è®¡æ•°
        if "prompt" in line_lower and "token" in line_lower:
            # å°è¯•æå–æ•°å­—
            import re
            numbers = re.findall(r'\d+', line)
            if numbers:
                metrics["prompt_tokens"] = int(numbers[0])
        
        if "generated" in line_lower and "token" in line_lower:
            import re
            numbers = re.findall(r'\d+', line)
            if numbers:
                metrics["generated_tokens"] = int(numbers[0])
        
        # æŸ¥æ‰¾ tokens/s
        if "tokens/s" in line_lower or "tokens per second" in line_lower:
            import re
            numbers = re.findall(r'\d+\.?\d*', line)
            if numbers:
                metrics["generated_tokens_per_sec"] = float(numbers[0])
    
    # è®¡ç®—é€Ÿç‡ï¼ˆå¦‚æœæœªä»è¾“å‡ºä¸­æå–ï¼‰
    if metrics["prompt_tokens"] > 0 and elapsed > 0:
        metrics["prompt_tokens_per_sec"] = metrics["prompt_tokens"] / elapsed
    
    if metrics["generated_tokens"] > 0 and elapsed > 0:
        if metrics["generated_tokens_per_sec"] == 0:
            metrics["generated_tokens_per_sec"] = metrics["generated_tokens"] / elapsed
    
    metrics["total_tokens_per_sec"] = (
        metrics["prompt_tokens_per_sec"] + metrics["generated_tokens_per_sec"]
    )
    
    return metrics


def print_metrics(metrics: Dict):
    """æ‰“å°æ€§èƒ½æŒ‡æ ‡"""
    if "error" in metrics:
        print(f"   é”™è¯¯: {metrics['error']}")
        return
    
    print(f"   æç¤ºè¯ tokens: {metrics['prompt_tokens']}")
    print(f"   ç”Ÿæˆ tokens: {metrics['generated_tokens']}")
    print(f"   æç¤ºè¯é€Ÿåº¦: {metrics['prompt_tokens_per_sec']:.2f} t/s")
    print(f"   ç”Ÿæˆé€Ÿåº¦: {metrics['generated_tokens_per_sec']:.2f} t/s")
    print(f"   æ€»è€—æ—¶: {metrics['elapsed_time']:.2f} ç§’")


def run_all_experiments() -> List[Dict]:
    """è¿è¡Œæ‰€æœ‰å®éªŒ"""
    results = []
    
    # åŸºå‡†çº¿: é»˜è®¤é…ç½®
    results.append(run_benchmark(
        name="Baseline (é»˜è®¤é…ç½®)",
        threads=4,
        kv_cache_type=None,
        batch_size=512
    ))
    
    # å®éªŒ A: çº¿ç¨‹ç¼©æ”¾
    for threads in [4, 6, 8]:
        results.append(run_benchmark(
            name=f"å®éªŒA-{threads}çº¿ç¨‹",
            threads=threads,
            kv_cache_type=None,
            batch_size=512
        ))
    
    # å®éªŒ B: KV Cache é‡åŒ–
    results.append(run_benchmark(
        name="å®éªŒB-KVé‡åŒ–(q8_0)",
        threads=6,
        kv_cache_type="q8_0",
        batch_size=512
    ))
    
    # å®éªŒ C: æ‰¹å¤„ç†å¤§å°
    for batch_size in [256, 512, 1024]:
        results.append(run_benchmark(
            name=f"å®éªŒC-æ‰¹å¤„ç†{batch_size}",
            threads=6,
            kv_cache_type="q8_0",
            batch_size=batch_size
        ))
    
    # å®éªŒ D: æŠ•æœºé‡‡æ · (å¦‚æœ Draft Model å­˜åœ¨)
    if DRAFT_MODEL.exists():
        results.append(run_benchmark(
            name="å®éªŒD-æŠ•æœºé‡‡æ ·",
            threads=6,
            kv_cache_type="q8_0",
            batch_size=512,
            draft_model=DRAFT_MODEL
        ))
    else:
        print(f"\nâš ï¸  è·³è¿‡å®éªŒD: Draft Model æœªæ‰¾åˆ° ({DRAFT_MODEL})")
    
    return results


def save_results(results: List[Dict]):
    """ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = LOGS_DIR / f"benchmark_{timestamp}.json"
    
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {log_file}")
    
    # åŒæ—¶ç”Ÿæˆ Markdown æŠ¥å‘Š
    md_file = LOGS_DIR / f"benchmark_{timestamp}.md"
    generate_markdown_report(results, md_file)
    print(f"ğŸ“„ Markdown æŠ¥å‘Šå·²ä¿å­˜åˆ°: {md_file}")


def generate_markdown_report(results: List[Dict], output_file: Path):
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š"""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# LocalLLM_Lab åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## å®éªŒç»“æœ\n\n")
        f.write("| å®éªŒåç§° | æç¤ºè¯é€Ÿåº¦ (t/s) | ç”Ÿæˆé€Ÿåº¦ (t/s) | æ€»é€Ÿåº¦ (t/s) |\n")
        f.write("|---------|----------------|--------------|------------|\n")
        
        for r in results:
            if "error" not in r:
                f.write(f"| {r['name']} | "
                       f"{r['prompt_tokens_per_sec']:.2f} | "
                       f"{r['generated_tokens_per_sec']:.2f} | "
                       f"{r['total_tokens_per_sec']:.2f} |\n")
            else:
                f.write(f"| {r['name']} | âŒ {r['error']} | - | - |\n")
        
        f.write("\n## è¯¦ç»†æ•°æ®\n\n")
        f.write("```json\n")
        f.write(json.dumps(results, indent=2, ensure_ascii=False))
        f.write("\n```\n")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LocalLLM_Lab - åŸºå‡†æµ‹è¯•å¼€å§‹")
    print(f"   é¡¹ç›®ç›®å½•: {PROJECT_ROOT}")
    
    ensure_dirs()
    
    if not check_prerequisites():
        sys.exit(1)
    
    print(f"\nâœ… å‰ç½®æ¡ä»¶æ£€æŸ¥é€šè¿‡")
    print(f"   å¯æ‰§è¡Œæ–‡ä»¶: {LLAMA_CLI}")
    print(f"   ä¸»æ¨¡å‹: {MAIN_MODEL}")
    print(f"   Draft æ¨¡å‹: {DRAFT_MODEL} ({'å­˜åœ¨' if DRAFT_MODEL.exists() else 'ä¸å­˜åœ¨'})")
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    results = run_all_experiments()
    
    # ä¿å­˜ç»“æœ
    save_results(results)
    
    print("\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼")


if __name__ == "__main__":
    main()

